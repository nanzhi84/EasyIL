seed: 42

algo:
  name: sac
  policy: MlpPolicy

  kwargs:
    learning_rate: 3e-4
    qf_learning_rate: null  # Q-function learning rate, null = use learning_rate
    buffer_size: 1000000
    learning_starts: 10000
    batch_size: 1024
    tau: 0.005
    gamma: 0.99

    train_freq: 1
    gradient_steps: 8
    policy_delay: 1

    ent_coef: auto
    target_entropy: auto

    verbose: 1
    seed: ${seed}
    device: auto

    policy_kwargs: null  # Extra policy network kwargs

train:
  mode: online

  total_timesteps: 1000000

  log_freq: 10000
  eval_freq: 50000
  n_eval_episodes: 10
  deterministic_eval: true

  save_best: true
  checkpoint_freq: 250000

  progress_bar: true
  log_interval: 1000

  plot_enabled: true
  plot_every_eval: true

  final_eval_episodes: 10
  final_deterministic_eval: true

env:
  id: HalfCheetah-v5
  num_envs: 64
  eval_num_envs: 64
  kwargs: {}

  normalize:
    enabled: false
    norm_obs: true
    norm_reward: false
    clip_obs: 10.0
    clip_reward: 10.0

  reward:
    model_path: null
    hidden_dim: 256
    scale: 1.0

  compare_reward: null

logger:
  enabled: false
  type: null

hydra:
  run:
    dir: outputs/${env.id}/${algo.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${env.id}/${algo.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
